{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d5d681-bfee-49b2-84b0-97f735513994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# !pip install matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4e67a1-0f62-4092-a614-72f18b495c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.2\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42fcd001-f49d-4609-9a82-7b8d6f5cf6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0.dev20250306+cu128'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ae9a36-d1c9-42c5-9036-becd12e282ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Laptop GPU'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88c1c044-d606-494d-9c1d-40f5e483ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download link:  https://drive.google.com/uc?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb80068-6074-448d-a32c-d726c90c8cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get all image file paths from the directory\n",
    "        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir) if img.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply the transform if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686b7d62-ecaa-452b-95a9-b81ff78b087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images loaded: 202599\n"
     ]
    }
   ],
   "source": [
    "# Define transformations (resize, crop, convert to tensor, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),  # Resize images to 64x64\n",
    "    transforms.CenterCrop(64),  # Crop center to 64x64\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load CelebA dataset from the specified directory\n",
    "dataset_path = 'img_align_celeba'\n",
    "dataset = CelebADataset(root_dir=dataset_path, transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Check how many images are loaded\n",
    "print(f\"Total number of images loaded: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82879dad-bd28-45c1-a130-69f4ebf7ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Generator and Discriminator classes (same as previously described)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=100, img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, img_channels * 64 * 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 3, 64, 64)  # Reshape to image format\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(img_channels * 64 * 64, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c0a2ac-f17d-4104-8915-327428229c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizers\n",
    "adversarial_loss = nn.BCELoss()\n",
    "generator = Generator(z_dim=100)\n",
    "discriminator = Discriminator()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e78dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(generator, discriminator, dataloader, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        for i, imgs in enumerate(dataloader):\n",
    "            real_imgs = imgs.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "            valid = torch.ones(batch_size, 1).to(device)\n",
    "            fake = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(generator(torch.randn(batch_size, 100).to(device)).detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss = adversarial_loss(discriminator(generator(torch.randn(batch_size, 100).to(device))), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "        # Optionally, save generated images at each epoch\n",
    "        save_generated_images(generator, epoch, device)\n",
    "        \n",
    "def save_generated_images(generator, epoch, device, num_images=16):\n",
    "    z = torch.randn(num_images, 100).to(device)\n",
    "    generated_imgs = generator(z).detach().cpu()\n",
    "    grid = torchvision.utils.make_grid(generated_imgs, nrow=4, normalize=True)\n",
    "    plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "    plt.title(f\"Epoch {epoch}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37806814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "train(generator, discriminator, dataloader, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
